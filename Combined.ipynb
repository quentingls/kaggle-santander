{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_path = r'/Users/user/Documents/kaggle/santander/'\n",
    "train_rpath = r'data/train.csv'\n",
    "test_rpath = r'data/test.csv'\n",
    "\n",
    "sample_rpath = r'data/sample_submission.csv'\n",
    "output_rpath = r'output/combined.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(master_path + train_rpath)\n",
    "test_df = pd.read_csv(master_path + test_rpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data\n",
    "X_train_raw = train_df.drop([\"ID\", \"target\"], axis=1)\n",
    "# log since huge difference in orders\n",
    "# TODO check we reverse this\n",
    "\n",
    "y_train = np.log1p(train_df[\"target\"].values)\n",
    "# y_train = train_df[\"target\"].values\n",
    "\n",
    "# there is no target in test\n",
    "X_test_raw = test_df.drop([\"ID\"], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def summary(df):\n",
    "#     n_data, n_features = df.shape[0], df.shape[1]\n",
    "#     print (\"Number of Records: {}\".format(n_data))\n",
    "#     print (\"Number of Features: {}\".format(n_features))\n",
    "#     print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def missingVal(df):\n",
    "#     print(\"Total Train Features with NaN Values = \" + str(df.columns[df.isnull().sum() != 0].size))\n",
    "#     if (df.columns[df.isnull().sum() != 0].size):\n",
    "#         print(\"Features with NaN => {}\".format(list(df.columns[train_df.isnull().sum() != 0])))\n",
    "#         df[df.columns[df.isnull().sum() != 0]].isnull().sum().sort_values(ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://github.com/pandas-dev/pandas/issues/11250\n",
    "# def duplicate_columns(frame):\n",
    "#     groups = frame.columns.to_series().groupby(frame.dtypes).groups\n",
    "#     dups = []\n",
    "\n",
    "#     for t, v in groups.items():\n",
    "\n",
    "#         cs = frame[v].columns\n",
    "#         vs = frame[v]\n",
    "#         lcs = len(cs)\n",
    "\n",
    "#         for i in range(lcs):\n",
    "#             iv = vs.iloc[:,i].tolist()\n",
    "#             for j in range(i+1, lcs):\n",
    "#                 jv = vs.iloc[:,j].tolist()\n",
    "#                 if iv == jv:\n",
    "#                     dups.append(cs[i])\n",
    "#                     break\n",
    "\n",
    "#     return dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kn_feature_selection(x_train, x_test, p_threshold):\n",
    "    dropped_features = []\n",
    "    for feature in x_train:\n",
    "        res = stats.ks_2samp(x_train[feature], x_test[feature])\n",
    "        D, p = res\n",
    "#         print(\"D : %r,  p : %r\"%(D, p))\n",
    "        if p < p_threshold:\n",
    "            dropped_features.append(feature)\n",
    "    print(len(dropped_features))\n",
    "    return x_train.drop(dropped_features, axis=1), x_test.drop(dropped_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1408\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = kn_feature_selection(X_train_raw, X_test_raw, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new features\n",
    "# need to be careful so that aggregation is not used in next aggregation\n",
    "def new_feat_gen(df):\n",
    "    newFeat_train = pd.DataFrame()\n",
    "    newFeat_train['min'] = df.min(axis=1)\n",
    "    newFeat_train['max'] = df.max(axis=1)\n",
    "    newFeat_train['median'] = df.median(axis=1)\n",
    "    newFeat_train['q1'] = df.quantile(q=0.25, axis=1)\n",
    "    newFeat_train['q3'] = df.quantile(q=0.75, axis=1)\n",
    "    newFeat_train['sum'] = df.sum(axis=1)\n",
    "    newFeat_train['non-zero'] = df.apply(lambda x: x.nonzero()[0].size, axis=1)\n",
    "    newFeat_train['mean_non'] = newFeat_train['sum'] / newFeat_train['non-zero']\n",
    "#     print(df.shape)\n",
    "#     print(newFeat_train.shape)\n",
    "    return pd.concat([df, newFeat_train], axis=1, ignore_index=False,sort=False)\n",
    "#     return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = new_feat_gen(X_train)\n",
    "X_test = new_feat_gen(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def topFeatures(model, X_train, y_train, title):\n",
    "#     model.fit(X_train, y_train)\n",
    "#     print(model)\n",
    "\n",
    "#     feat_importances = pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "#     feat_importances = feat_importances.nlargest(25)\n",
    "#     plt.figure(figsize=(16,8))\n",
    "#     feat_importances.plot(kind='barh')\n",
    "#     plt.gca().invert_yaxis()\n",
    "#     plt.title(title)\n",
    "#     plt.show()\n",
    "#     return feat_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common_features_int = ['58232a6fb', '9fd594eec', '15ace8c9f', 'f190486d6', '58e2e02e6', 'eeb9cd3aa']\n",
    "\n",
    "\n",
    "# # Data visualisation prep\n",
    "# df_plot_pp = X_train[common_features_int]\n",
    "# df_plot_pp['target'] = y_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # common_features_union = pd.Series(list(set(s1).union(set(s2)))).values\n",
    "# common_features_union = ['b43a7cfd5', '994b4c2ac', 'ced6a7e91', 'fb0f5dbfe', 'f190486d6', '58e2e02e6',\n",
    "#                          '6cf7866c1', 'bc70cbc26', 'd6bb78916', 'c47340d97', '66ace2992', '58e056e12',\n",
    "#                          '6eef030c1', '4af7c76b9', '2288333b4', '9fd594eec', '58232a6fb', '024c577b9',\n",
    "#                          '609784003', '2a83c3267', '15ace8c9f', '402b0d650', 'd48c08bda', 'e222309b0',\n",
    "#                          '20aa07010', '1702b5bf0', 'eeb9cd3aa', '1931ccfdd', '6786ea46d']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PCA\n",
    "# # why only 3, lets try 5, 10 etc\n",
    "# pca = PCA(n_components=100)\n",
    "# pca.fit(X_train)\n",
    "# X_reduced = pca.transform(X_train)\n",
    "# X_reduced_test = pca.transform(X_test)\n",
    "# print(pca.explained_variance_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# development and validation\n",
    "# TODO use k-folds validation?\n",
    "dev_X, val_X, dev_y, val_y = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# TODO does PCA with a tree make sense?\n",
    "# TODO get book and build pipeline\n",
    "# TODO cv parameters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see difference with XGboost compared to light model\n",
    "# TODO check that the correct metric is being used\n",
    "def run_lgb(train_X, train_y, val_X, val_y, test_X):\n",
    "    params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"rmse\",\n",
    "        \"num_leaves\": 40,\n",
    "        \"learning_rate\": 0.005,\n",
    "        \"bagging_fraction\": 0.7,\n",
    "        \"feature_fraction\": 0.5,\n",
    "        \"bagging_frequency\": 5,\n",
    "        \"bagging_seed\": 42,\n",
    "        \"verbosity\": -1,\n",
    "        \"seed\": 42,\n",
    "\n",
    "#         'boosting': 'gbdt',\n",
    "#         'learning_rate': 0.01,\n",
    "#         'num_leaves': 32,\n",
    "#         'max_depth': 8,\n",
    "#         'feature_fraction': 0.7\n",
    "}\n",
    "    \n",
    "    \n",
    "    \n",
    "    lgtrain = lgb.Dataset(train_X, label=train_y)\n",
    "    lgval = lgb.Dataset(val_X, label=val_y)\n",
    "    evals_result = {}\n",
    "    model = lgb.train(params, lgtrain, 5000,\n",
    "                      valid_sets=[lgval],\n",
    "                      early_stopping_rounds=100,\n",
    "                      verbose_eval=50,\n",
    "                      evals_result=evals_result)\n",
    "\n",
    "# reverse log operation\n",
    "#     pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n",
    "    pred_test_y = np.expm1(model.predict(test_X, num_iteration=model.best_iteration))\n",
    "    return pred_test_y, model, evals_result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's rmse: 1.61914\n",
      "[100]\tvalid_0's rmse: 1.57077\n",
      "[150]\tvalid_0's rmse: 1.54043\n",
      "[200]\tvalid_0's rmse: 1.51997\n",
      "[250]\tvalid_0's rmse: 1.50449\n",
      "[300]\tvalid_0's rmse: 1.49475\n",
      "[350]\tvalid_0's rmse: 1.48906\n",
      "[400]\tvalid_0's rmse: 1.48447\n",
      "[450]\tvalid_0's rmse: 1.48265\n",
      "[500]\tvalid_0's rmse: 1.48148\n",
      "[550]\tvalid_0's rmse: 1.48135\n",
      "[600]\tvalid_0's rmse: 1.48132\n",
      "Early stopping, best iteration is:\n",
      "[531]\tvalid_0's rmse: 1.48092\n",
      "LightGBM Training Completed...\n"
     ]
    }
   ],
   "source": [
    "yhat_lgbm, model_lgbm, evals_result_lgbm = run_lgb(dev_X, dev_y, val_X, val_y, X_test)\n",
    "print(\"LightGBM Training Completed...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see difference with XGboost compared to light model\n",
    "# TODO check that the correct metric is being used\n",
    "def run_lgb2(train_X, train_y, val_X, val_y, test_X):\n",
    "    params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"rmse\",\n",
    "        \"num_leaves\": 30,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"bagging_fraction\": 0.7,\n",
    "        \"feature_fraction\": 0.4,\n",
    "        \"bagging_frequency\": 5,\n",
    "        \"bagging_seed\": 42,\n",
    "        \"verbosity\": -1,\n",
    "        \"seed\": 42,\n",
    "\n",
    "#         'boosting': 'gbdt',\n",
    "#         'learning_rate': 0.01,\n",
    "#         'num_leaves': 32,\n",
    "#         'max_depth': 8,\n",
    "#         'feature_fraction': 0.7\n",
    "}\n",
    "    \n",
    "    \n",
    "    \n",
    "    lgtrain = lgb.Dataset(train_X, label=train_y)\n",
    "    lgval = lgb.Dataset(val_X, label=val_y)\n",
    "    evals_result = {}\n",
    "    model = lgb.train(params, lgtrain, 5000,\n",
    "                      valid_sets=[lgval],\n",
    "                      early_stopping_rounds=100,\n",
    "                      verbose_eval=50,\n",
    "                      evals_result=evals_result)\n",
    "\n",
    "# reverse log operation\n",
    "#     pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n",
    "    pred_test_y = np.expm1(model.predict(test_X, num_iteration=model.best_iteration))\n",
    "    return pred_test_y, model, evals_result\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's rmse: 1.57989\n",
      "[100]\tvalid_0's rmse: 1.52856\n",
      "[150]\tvalid_0's rmse: 1.50546\n",
      "[200]\tvalid_0's rmse: 1.49434\n",
      "[250]\tvalid_0's rmse: 1.48861\n",
      "[300]\tvalid_0's rmse: 1.48568\n",
      "[350]\tvalid_0's rmse: 1.4862\n",
      "[400]\tvalid_0's rmse: 1.4876\n",
      "Early stopping, best iteration is:\n",
      "[314]\tvalid_0's rmse: 1.48522\n",
      "LightGBM Training Completed2...\n"
     ]
    }
   ],
   "source": [
    "yhat_lgbm2, model_lgbm2, evals_result_lgbm2 = run_lgb2(dev_X, dev_y, val_X, val_y, X_test)\n",
    "print(\"LightGBM Training Completed2...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # feature importance\n",
    "# print(\"Features Importance...\")\n",
    "# gain = model.feature_importance('gain')\n",
    "# featureimp = pd.DataFrame({'feature':model.feature_name(),\n",
    "#                    'split':model.feature_importance('split'),\n",
    "#                    'gain':100 * gain / gain.sum()}).sort_values('gain', ascending=False)\n",
    "# print(featureimp[:15])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_xgb(train_X, train_y, val_X, val_y, test_X):\n",
    "    params = {'objective': 'reg:linear', \n",
    "      'eval_metric': 'rmse',\n",
    "      'eta': 0.001,\n",
    "      'max_depth': 10, \n",
    "      'subsample': 0.6, \n",
    "      'colsample_bytree': 0.6,\n",
    "      'alpha':0.001,\n",
    "      'random_state': 42, \n",
    "      'silent': True}\n",
    "    \n",
    "    xgtrain = xgb.DMatrix(train_X, train_y)\n",
    "    xgvalid = xgb.DMatrix(val_X, val_y)\n",
    "    \n",
    "    eval_results = [(xgtrain, 'train'), (xgvalid, 'valid')]\n",
    "    \n",
    "    model = xgb.train(params, xgtrain, 5000, eval_results, \n",
    "                      maximize=False, \n",
    "                      verbose_eval=100, \n",
    "                      early_stopping_rounds=100)\n",
    "    \n",
    "    xgtest_x = xgb.DMatrix(test_X)\n",
    "    x_hat_test = np.expm1(model.predict(xgtest_x, ntree_limit=model.best_ntree_limit))\n",
    "    return x_hat_test, model, eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:14.0877\tvalid-rmse:14.0768\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 100 rounds.\n",
      "[100]\ttrain-rmse:12.7659\tvalid-rmse:12.7558\n",
      "[200]\ttrain-rmse:11.571\tvalid-rmse:11.5622\n",
      "[300]\ttrain-rmse:10.4915\tvalid-rmse:10.4837\n",
      "[400]\ttrain-rmse:9.51596\tvalid-rmse:9.51037\n",
      "[500]\ttrain-rmse:8.63476\tvalid-rmse:8.63133\n",
      "[600]\ttrain-rmse:7.83891\tvalid-rmse:7.83792\n",
      "[700]\ttrain-rmse:7.11998\tvalid-rmse:7.12164\n",
      "[800]\ttrain-rmse:6.47103\tvalid-rmse:6.47542\n",
      "[900]\ttrain-rmse:5.88532\tvalid-rmse:5.89314\n",
      "[1000]\ttrain-rmse:5.35686\tvalid-rmse:5.36854\n",
      "[1100]\ttrain-rmse:4.88008\tvalid-rmse:4.89613\n",
      "[1200]\ttrain-rmse:4.45076\tvalid-rmse:4.47208\n",
      "[1300]\ttrain-rmse:4.06395\tvalid-rmse:4.09182\n",
      "[1400]\ttrain-rmse:3.71542\tvalid-rmse:3.75119\n",
      "[1500]\ttrain-rmse:3.40184\tvalid-rmse:3.44675\n",
      "[1600]\ttrain-rmse:3.12025\tvalid-rmse:3.17532\n",
      "[1700]\ttrain-rmse:2.86774\tvalid-rmse:2.93435\n",
      "[1800]\ttrain-rmse:2.64092\tvalid-rmse:2.72038\n",
      "[1900]\ttrain-rmse:2.43785\tvalid-rmse:2.53132\n",
      "[2000]\ttrain-rmse:2.25608\tvalid-rmse:2.36459\n",
      "[2100]\ttrain-rmse:2.09393\tvalid-rmse:2.21849\n",
      "[2200]\ttrain-rmse:1.94923\tvalid-rmse:2.09115\n",
      "[2300]\ttrain-rmse:1.82086\tvalid-rmse:1.98066\n",
      "[2400]\ttrain-rmse:1.70663\tvalid-rmse:1.88514\n",
      "[2500]\ttrain-rmse:1.60533\tvalid-rmse:1.80277\n",
      "[2600]\ttrain-rmse:1.51572\tvalid-rmse:1.73206\n",
      "[2700]\ttrain-rmse:1.43574\tvalid-rmse:1.67137\n",
      "[2800]\ttrain-rmse:1.3657\tvalid-rmse:1.62031\n",
      "[2900]\ttrain-rmse:1.3041\tvalid-rmse:1.57692\n",
      "[3000]\ttrain-rmse:1.24972\tvalid-rmse:1.54072\n",
      "[3100]\ttrain-rmse:1.20237\tvalid-rmse:1.5102\n",
      "[3200]\ttrain-rmse:1.16064\tvalid-rmse:1.48453\n",
      "[3300]\ttrain-rmse:1.12393\tvalid-rmse:1.46314\n",
      "[3400]\ttrain-rmse:1.09099\tvalid-rmse:1.44526\n",
      "[3500]\ttrain-rmse:1.06211\tvalid-rmse:1.4304\n",
      "[3600]\ttrain-rmse:1.0372\tvalid-rmse:1.41822\n",
      "[3700]\ttrain-rmse:1.01468\tvalid-rmse:1.40811\n",
      "[3800]\ttrain-rmse:0.99475\tvalid-rmse:1.39982\n",
      "[3900]\ttrain-rmse:0.976447\tvalid-rmse:1.39284\n",
      "[4000]\ttrain-rmse:0.960412\tvalid-rmse:1.38712\n",
      "[4100]\ttrain-rmse:0.946031\tvalid-rmse:1.3824\n",
      "[4200]\ttrain-rmse:0.933276\tvalid-rmse:1.37856\n",
      "[4300]\ttrain-rmse:0.921864\tvalid-rmse:1.3754\n",
      "[4400]\ttrain-rmse:0.910923\tvalid-rmse:1.37259\n",
      "[4500]\ttrain-rmse:0.900995\tvalid-rmse:1.37053\n",
      "[4600]\ttrain-rmse:0.892034\tvalid-rmse:1.36876\n",
      "[4700]\ttrain-rmse:0.883209\tvalid-rmse:1.3672\n",
      "[4800]\ttrain-rmse:0.875235\tvalid-rmse:1.36626\n",
      "[4900]\ttrain-rmse:0.867642\tvalid-rmse:1.36531\n",
      "[4999]\ttrain-rmse:0.860912\tvalid-rmse:1.36448\n"
     ]
    }
   ],
   "source": [
    "yhat_xgb, model_xgb, eval_results_xgb = run_xgb(dev_X, dev_y, val_X, val_y, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "outfile = open(r'/Users/user/Documents/kaggle/santander/xgb','wb')\n",
    "pickle.dump(model_xgb,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = {'lgbm': 1.34956, 'xgb': 1.36448}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(master_path + sample_rpath)\n",
    "\n",
    "submission_raw = pd.DataFrame()\n",
    "submission_raw[\"lgbm\"] = yhat_lgbm\n",
    "submission_raw[\"lgbm2\"] = yhat_lgbm2\n",
    "submission_raw[\"xgb\"] = yhat_xgb\n",
    "submission_raw[\"target\"] = submission_raw.mean(axis=1)\n",
    "\n",
    "submission[\"target\"] = submission_raw[\"target\"] \n",
    "# submission[\"target\"] = (submission_raw[\"lgbm\"] * 0.5 + submission_raw[\"xgb\"] * 0.5)\n",
    "# submission[\"target\"] = (submission_raw[\"lgbm\"] / weights['lgbm'] + submission_raw[\"xgb\"]  / weights['xgb']) * (weights['lgbm'] + weights['xgb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lgbm</th>\n",
       "      <th>lgbm2</th>\n",
       "      <th>xgb</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.216422e+06</td>\n",
       "      <td>2.123589e+06</td>\n",
       "      <td>4531137.00</td>\n",
       "      <td>2.957049e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.526226e+06</td>\n",
       "      <td>2.482669e+06</td>\n",
       "      <td>2878040.50</td>\n",
       "      <td>2.628979e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.841526e+06</td>\n",
       "      <td>1.847687e+06</td>\n",
       "      <td>2571857.75</td>\n",
       "      <td>2.087024e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.168351e+06</td>\n",
       "      <td>2.149097e+06</td>\n",
       "      <td>4510288.00</td>\n",
       "      <td>2.942579e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.601213e+06</td>\n",
       "      <td>2.614572e+06</td>\n",
       "      <td>2777751.00</td>\n",
       "      <td>2.664512e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           lgbm         lgbm2         xgb        target\n",
       "0  2.216422e+06  2.123589e+06  4531137.00  2.957049e+06\n",
       "1  2.526226e+06  2.482669e+06  2878040.50  2.628979e+06\n",
       "2  1.841526e+06  1.847687e+06  2571857.75  2.087024e+06\n",
       "3  2.168351e+06  2.149097e+06  4510288.00  2.942579e+06\n",
       "4  2.601213e+06  2.614572e+06  2777751.00  2.664512e+06"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ID        target\n",
      "0  000137c73  2.957049e+06\n",
      "1  00021489f  2.628979e+06\n",
      "2  0004d7953  2.087024e+06\n",
      "3  00056a333  2.942579e+06\n",
      "4  00056d8eb  2.664512e+06\n"
     ]
    }
   ],
   "source": [
    "# save results\n",
    "print(submission.head())\n",
    "submission.to_csv(master_path + output_rpath, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
